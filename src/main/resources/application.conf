
app {

  configureDefaultPartitions = true

  timePeriod = "201804"

  environment = "local"
  payeFilePath = "src/test/resources/data/newperiod/newPeriodPaye.csv"
  vatFilePath = "src/test/resources/data/newperiod/newPeriodVat.csv"
  BIFilePath = "src/main/resources/data/newperiod/newPeriod.json"
  parquetFilePath = "src/test/resources/data/newperiod/sample.parquet"
  createParquetFile = false

  defaultPRN = "0"
  defaultWorkingProps = "0"

  sequence {
    url = "localhost:2181"
    resultFormat = "11%08d"
    path = "/ids/enterprise/id"
    sessionTimeout = 5
    connectionTimeout = 5
  }

  geo {
    default = "src/test/resources/data/geo/test-dataset.csv"
    defaultShort = "src/test/resources/data/geo/test_short-dataset.csv"
    pathToGeo = "src/test/resources/data/geo/test-dataset.csv"
    pathToGeoShort = "src/test/resources/data/geo/test_short-dataset.csv"
  }

  links {
    namespace = "ons"
    tableName = "LINKS"
    columnFamily = "l"
    filePath = "src/test/resources/data/newperiod/links"
  }

  admindata {
    namespace = "ons"
    tableName = "ADMINDATA"
    columnFamily = "d"
    filePath = "src/test/resources/data/newperiod/admindata"
  }

  region {
    namespace = "ons"
    tablename = "REGION"
    columnFamily = "d"
    filePath = "src/test/resources/data/newperiod/region"
  }

  employment {
    namespace = "ons"
    tablename = "EMPLOYMENT"
    columnFamily = "d"
    filePath = "src/test/resources/data/newperiod/employment"
  }
}

hadoop.security.authentication = "kerberos"

spark.sql.broadcastTimeout = 2400

files {
  env.config = ${?envconf}
  parquet = "src/test/resources/data/sample.parquet"
  links.hfile = "src/test/resources/data/links/hfile"
  admindata.hfile = "src/test/resources/data/admindata/hfile"
  region.hfile = "src/test/resources/data/region/hfile"
  employment.hfile = "src/test/resources/data/employment/hfile"
}

hbase {
  rpc.timeout = 360000
  client.scanner.timeout.period = 360000
  cells.scanned.per.heartbeat.check = 60000
  zookeeper.property.clientPort = 2181
  zookeeper.quorum = "localhost"
  mapreduce.bulkload.max.hfiles.perRegion.perFamily = 500
  hbase.rpc.timeout = 360000
  hbase.client.scanner.timeout.period = 360000
  hbase.cells.scanned.per.heartbeat.check = 60000
  security.authentication = "kerberos"
  files.per.region = 500
  path.config = ${?hbaseconf}  # path to hbase config resource, i.e. hbase-site.xml
  kerberos.config = ${?kerberosconf}

  table {
    links {
      name = "LINKS"
      column.family = "l"
      namespace = "ons"
    }

    admindata {
      name = "ADMINDATA"
      column.family = "d"
      namespace = "ons"
    }

    region {
      name = "REGION"
      column.family = "d"
      namespace = "ons"
    }

    employment {
      name = "EMPLOYMENT"
      column.family = "d"
      namespace = "ons"
    }
  }
}

enterprise {
  data.timeperiod = "201801"
}


# Don't create _SUCCESS files so we don't see the 'Skipping non-directory file' warning
# on load into hbase

mapreduce {
  fileoutputcommitter.marksuccessfuljobs = false
}
